{"cells":[{"cell_type":"markdown","metadata":{"id":"reE9mtcJgoHy"},"source":["# Introduction to CNN\n","\n","The problem is about **classifying grayscale images of handwritten digits** (28 pixels by 28 pixels), into their 10 categories (0 to 9), exactly like the one in chapter 2. This time will try to solve the problem through a convolutional neural network (CNN) and see if performance improves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF43wDIIgoH5"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from tensorflow.keras import models, layers, optimizers, losses, metrics\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{"id":"V5BQEoDvgoIC"},"source":["## Build the network\n","\n","A basic convnet will be used: a stack of `Conv2D` and `MaxPooling2D` layers.\n","Importantly, a CNN takes as input tensors of shape (`image_height, image_width, image_channels`), not including the batch dimensions. In this case, we have to configure the CNN to process images of a size compatible with the MNIST database, so it will be of size (28, 28, 1). This will be the input shape to pass to the first layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4ro-1KRgoID","outputId":"afdc6d53-a9e1-4e00-82ef-bc5e796ee3ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Sv2qSu5ogoIF"},"source":["The output of every `Conv2D` and `MaxPooling2D` layer is also a tensor of shape (`height, width, channels`). Interesting to note how the dimensions tend to shrink as going deeper in the network. The channel is controlled by the first parameter in `Conv2D` layers.\n","\n","While the the 3rd parameter of the input shape indicates the color channels, in the output of a layer it indicates the number of filters (features) over its input. So, every dimension in the depth axis is a feature (filter), and the 2D tensor is a 2D spatial map of the response of this filter over the input."]},{"cell_type":"markdown","metadata":{"id":"4OBkE5yGgoIH"},"source":["#### Insert a classifier on top of the CNN\n","\n","This is a classification problem, so we need to put the last output tensor of the CNN as input to a densely connected network, similar to the one in chapter 2. We have a class of 10 categories, so the solution will be a 10-way classification, using a final layer of 10 outputs and a `softmax` activation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdfuHugqgoIJ","outputId":"11927977-147f-4c9d-8a23-c0b07fd458f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 576)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                36928     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"VqFgtgh3goIJ"},"source":["Note that the `Flatten` layer produces a vector of shape (`3 * 3 * 64`), which is the output of the last `Conv2D` layer. Operation needed before going through the 2 dense layers"]},{"cell_type":"markdown","metadata":{"id":"ttgHxaJBgoIK"},"source":["## Load dataset and preprocess data\n","\n","`train_images` and `train_labels` form the \"training set\", the data that the model will learn from. The model will then be tested on the \"test set\", `test_images` and `test_labels`. The images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n","\n","Before training, we will preprocess our data by reshaping it into the shape that the CNN expects. We do also need to categorically encode the labels (one-hot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uC59cWergoIL"},"outputs":[],"source":["(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L36EVY_ngoIM"},"outputs":[],"source":["train_images = train_images.reshape((60000, 28, 28, 1))\n","train_images = train_images.astype('float32') / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcOcvwKdgoIQ"},"outputs":[],"source":["test_images = test_images.reshape((10000, 28, 28, 1))\n","test_images = test_images.astype('float32') / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7JBkIR5goIS"},"outputs":[],"source":["train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)"]},{"cell_type":"markdown","metadata":{"id":"ZFSl-HVqgoIT"},"source":["## Compile and train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMu1wnBmgoIT","outputId":"19c54c26-8473-4882-bd49-b09ad029b397"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 60000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 18s 305us/sample - loss: 0.1729 - accuracy: 0.9465\n","Epoch 2/5\n","60000/60000 [==============================] - 18s 296us/sample - loss: 0.0486 - accuracy: 0.9852\n","Epoch 3/5\n","60000/60000 [==============================] - 18s 295us/sample - loss: 0.0331 - accuracy: 0.9900\n","Epoch 4/5\n","60000/60000 [==============================] - 18s 306us/sample - loss: 0.0254 - accuracy: 0.9923\n","Epoch 5/5\n","60000/60000 [==============================] - 18s 305us/sample - loss: 0.0194 - accuracy: 0.9938\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x1499b76a0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(\n","    optimizer='rmsprop',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","model.fit(train_images, train_labels, epochs=5, batch_size=64)"]},{"cell_type":"markdown","metadata":{"id":"oS-uB8y8goIU"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIhHuNZugoIU","outputId":"2753f71a-8958-45ba-ddd1-56a927ff0eef"},"outputs":[{"name":"stdout","output_type":"stream","text":["10000/10000 [==============================] - 1s 88us/sample - loss: 0.0287 - accuracy: 0.9909\n"]}],"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stvjv1EwgoIU","outputId":"d763b430-d75e-473b-83c8-7eba6c1276f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["test_acc: 0.9909\n"]}],"source":["print('test_acc:', test_acc)"]},{"cell_type":"markdown","metadata":{"id":"ky9VeBQCgoIV"},"source":["The test set accuracy turns out to be *99.1%*, which is quite an improvement from the fully connected network in chapter 2, which reached *97.8%* accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFkA9Wl7goIV"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"01 - Introduction to CNN.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}